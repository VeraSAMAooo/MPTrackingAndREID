{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import imagenet\n",
    "from tensorflow.contrib.slim.nets import inception\n",
    "import inception_preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "#batch_size = 3\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "\n",
    "img_list = ['/home/tiago/Desktop/img.jpeg', '/home/tiago/Desktop/img2.jpeg', '/home/tiago/Desktop/img3.jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]\n",
      "\n",
      "Caused by op 'input_producer/input_producer_EnqueueMany', defined at:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-f9fcaf7def3e>\", line 2, in <module>\n",
      "    filename_queue = tf.train.string_input_producer(img_list, shuffle=False)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n",
      "    cancel_op=cancel_op)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/input.py\", line 162, in input_producer\n",
      "    enq = q.enqueue_many([input_tensor])\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 371, in enqueue_many\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1212, in _queue_enqueue_many\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 147 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f9fcaf7def3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Probability %0.2f%% => [%s]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m#plt.imshow(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 147 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    filename_queue = tf.train.string_input_producer(img_list, shuffle=False)\n",
    "\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    image = tf.image.decode_jpeg(value, channels=3)\n",
    "    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n",
    "    processed_images = tf.expand_dims(processed_image, 0)\n",
    "\n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        logits, _ = inception.inception_v3(processed_images, num_classes=1001, is_training=False)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "    init_fn = slim.assign_from_checkpoint_fn('/home/tiago/PycharmProjects/DeepTrack/inception_v3.ckpt', slim.get_model_variables('InceptionV3'))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_fn(sess)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(3):\n",
    "            prb = probabilities.eval()\n",
    "\n",
    "            argmax = np.argmax(prb)\n",
    "            print('Probability %0.2f%% => [%s]' % (prb[argmax], names[argmax]))\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "            #np_image, probabilities = sess.run([image, probabilities])\n",
    "            #probabilities = probabilities[0, 0:]\n",
    "            #classification = np.argmax(probabilities)\n",
    "            #print('Probability %0.2f%% => [%s]' % (probabilities[classification], names[classification]))\n",
    "            #print(np_image.shape)\n",
    "            #plt.imshow(np_image)\n",
    "            #plt.show()\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00020923,  0.00017578,  0.00029098, ...,  0.00015783,\n",
       "         0.00022501,  0.00025338]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00020923,  0.00017578,  0.00029098, ...,  0.00015783,\n",
       "        0.00022501,  0.00025338], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020922985"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
